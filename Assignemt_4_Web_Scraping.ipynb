{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8396e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.  \n",
    "!pip install selenium\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "most_view=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody')\n",
    "\n",
    "for i in most_view:\n",
    "    temp_text=i.text\n",
    "    break\n",
    "print(temp_text)\n",
    "split=temp_text.split('\\n')\n",
    "split\n",
    "rank.clear()\n",
    "name.clear()\n",
    "artist.clear()\n",
    "udate.clear()\n",
    "views.clear()\n",
    "rank=[]\n",
    "name=[]\n",
    "artist=[]\n",
    "udate=[]\n",
    "views=[]\n",
    "for i in split:\n",
    "    rank.append(i.split('.')[0])\n",
    "    name.append(i.split('[')[0].split('.')[-1].strip())\n",
    "    artist.append(re.split('\\d+',i.split(']')[1])[0])\n",
    "    udate.append(str(re.compile('[a-zA-Z]+\\s+\\d{2}').findall(i.split('.')[2]))+','+str(re.compile('\\d{4}').findall(i.split('.')[2])))\n",
    "    views.append(str(re.compile('\\d+').findall(i.split(']')[1])[0])+'.'+str(re.compile('\\d+').findall(i.split(']')[1])[1]))\n",
    "print(rank)\n",
    "len(rank)\n",
    "print(name)\n",
    "print(artist)\n",
    "print(views)\n",
    "print(udate)\n",
    "print(len(rank),len(name),len(artist),len(views),len(udate))\n",
    "df=pd.DataFrame({'Rank':rank,'Name':name,'Artist':artist,'Upload Date':udate,'Views (billions)':views})\n",
    "df\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "2.\n",
    "!pip install selenium\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"http://www.bcci.tv/\")\n",
    "search=driver.find_element(By.XPATH,\"/html/body/nav/div[1]/div[2]/ul[1]/li[2]\")\n",
    "search.click()\n",
    "time.sleep(3)\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[2]/div[1]/div/ul/li[1]/a\")\n",
    "search.click()\n",
    "time.sleep(3)\n",
    "title=[]\n",
    "series=[]\n",
    "place=[]\n",
    "date=[]\n",
    "time=[]\n",
    "mtitle=driver.find_elements(By.XPATH,'//h5[@class=\"match-tournament-name ng-binding\"]')\n",
    "\n",
    "for i in mtitle:\n",
    "    mt=i.text\n",
    "    title.append(mt)\n",
    "    \n",
    "print(title)\n",
    "len(title)\n",
    "mseries=driver.find_elements(By.XPATH,'//span[@class=\"matchOrderText ng-binding ng-scope\"]')\n",
    "\n",
    "for i in mseries:\n",
    "    ms=i.text\n",
    "    series.append(ms)\n",
    "    \n",
    "print(series)\n",
    "len(series)\n",
    "mplace=driver.find_elements(By.XPATH,'//span[@class=\"ng-binding\"]')\n",
    "\n",
    "for i in mplace:\n",
    "    mp=i.text\n",
    "    place.append(mp)\n",
    "    \n",
    "print(place)\n",
    "mdate=driver.find_elements(By.XPATH,'//div[@class=\"match-dates ng-binding\"]')\n",
    "\n",
    "for i in mdate:\n",
    "    md=i.text\n",
    "    date.append(md)\n",
    "    \n",
    "print(date)\n",
    "mtime=driver.find_elements(By.XPATH,'//div[@class=\"match-time no-margin ng-binding\"]')\n",
    "\n",
    "for i in mtime:\n",
    "    mt=i.text\n",
    "    time.append(mt)\n",
    "    \n",
    "print(time)\n",
    "len(mtime)\n",
    "print(len(title),len(series),len(place),len(date),len(time))\n",
    "df=pd.DataFrame({'Match Title':title,'Series':series,'Place':place,'Date':date,'Time':time})\n",
    "df\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "3.\n",
    "\n",
    "!pip install selenium\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"http://statisticstimes.com/\")\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[2]/div[1]/div[2]/div[2]\")\n",
    "search.click()\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[2]/div[1]/div[2]/div[2]/div/a[3]\")\n",
    "search.click()\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\")\n",
    "search.click()\n",
    "rank=[]\n",
    "state=[]\n",
    "gsdp1819=[]\n",
    "gsdp1920=[]\n",
    "share=[]\n",
    "gdp=[]\n",
    "srank=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[1]')\n",
    "for i in srank:\n",
    "    rk=i.text\n",
    "    rank.append(rk)\n",
    "    \n",
    "print(rank)\n",
    "sstate=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[2]')\n",
    "\n",
    "for i in sstate:\n",
    "    st=i.text\n",
    "    state.append(st)\n",
    "    \n",
    "print(state)\n",
    "sgsdp1819=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[3]')\n",
    "\n",
    "for i in sgsdp1819:\n",
    "    gs=i.text\n",
    "    gsdp1819.append(gs)\n",
    "    \n",
    "print(gsdp1819)\n",
    "sgsdp1920=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[4]')\n",
    "\n",
    "for i in sgsdp1920:\n",
    "    gd=i.text\n",
    "    gsdp1920.append(gd)\n",
    "    \n",
    "print(gsdp1920)\n",
    "sshare=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[5]')\n",
    "\n",
    "for i in sshare:\n",
    "    sh=i.text\n",
    "    share.append(sh)\n",
    "    \n",
    "print(share)\n",
    "sgdp=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[6]')\n",
    "\n",
    "for i in sgdp:\n",
    "    sgd=i.text\n",
    "    gdp.append(sgd)\n",
    "    \n",
    "print(gdp)\n",
    "print(gsdp1819)\n",
    "print(len(rank),len(state),len(gsdp1819),len(gsdp1920),len(share),len(gdp))\n",
    "df=pd.DataFrame({'Rank':rank,'State':state,'GSDP(18-19) at current prices':gsdp1819,'GSDP(19-20) at current prices':gsdp1920,'Share (18-19)':share,'GDP($ billion)':gdp})\n",
    "df\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "4. \n",
    "\n",
    "!pip install selenium\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "driver=webdriver.Chrome()\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "driver.maximize_window()\n",
    "driver.get(\"https://github.com/\")\n",
    " \n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]\")\n",
    "search.click()\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a\")\n",
    "search.click()\n",
    "rep_title=[]\n",
    "des=[]\n",
    "count=[]\n",
    "lang=[]\n",
    "url=[]\n",
    "purl=driver.find_elements(By.XPATH,'//a[@class=\"Link\"]')\n",
    "\n",
    "for i in purl:\n",
    "    url.append(i.get_attribute('href'))\n",
    "\n",
    "time.sleep(3)\n",
    "print(url)\n",
    "len(url)\n",
    "rtitle=driver.find_elements(By.XPATH,'//h2[@class=\"h3 lh-condensed\"]')\n",
    "\n",
    "for i in rtitle:\n",
    "    rt=i.text\n",
    "    rep_title.append(rt)\n",
    "    \n",
    "print(rep_title)\n",
    "len(rep_title)\n",
    "rdes=driver.find_elements(By.XPATH,'//p[@class=\"col-9 color-fg-muted my-1 pr-4\"]')\n",
    "\n",
    "for i in rdes:\n",
    "    rd=i.text\n",
    "    des.append(rd)\n",
    "    \n",
    "print(des)\n",
    "for purl in url:\n",
    "    driver.get(purl)\n",
    "    time.sleep(1)\n",
    "\n",
    "    try:\n",
    "        rlan=driver.find_element(By.XPATH,'//div[@class=\"Layout-sidebar\"]/div/div[5]/div/ul/li/a//span[@class=\"color-fg-default text-bold mr-1\"]')\n",
    "        lang.append(rlan.text)\n",
    "        rcount=driver.find_element(By.XPATH,'//div[@class=\"Layout-sidebar\"]/div/div[4]/div/h2/a//span[@class=\"Counter ml-1\"]')\n",
    "        count.append(rcount.text)\n",
    "    except NoSuchElementException:\n",
    "        lang.append('-')\n",
    "        count.append('-')\n",
    "print(len(rep_title),len(des),len(count),len(lang))\n",
    "print(lang)\n",
    "df=pd.DataFrame({'Repository Title':rep_title,'Repositary Description':des,'Contributors Count':count})\n",
    "df\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "5.\n",
    "\n",
    "\n",
    "!pip install selenium\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.billboard.com/\")\n",
    "charts=driver.find_element(By.XPATH,\"/html/body/div[3]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a\")\n",
    "charts.click()\n",
    "view_charts=driver.find_element(By.XPATH,\"/html/body/div[3]/main/div[2]/div[1]/div[1]/div/div/div[3]/a\")\n",
    "view_charts.click()\n",
    "song=[]\n",
    "artist=[]\n",
    "last_week=[]\n",
    "peak=[]\n",
    "weeks=[]\n",
    "data=[]\n",
    "song_tag=driver.find_element(By.XPATH,'//h3[@class=\"c-title  a-no-trucate a-font-primary-bold-s u-letter-spacing-0021 u-font-size-23@tablet lrv-u-font-size-16 u-line-height-125 u-line-height-normal@mobile-max a-truncate-ellipsis u-max-width-245 u-max-width-230@tablet-only u-letter-spacing-0028@tablet\"]')\n",
    "song.append(song_tag.text)\n",
    "s_tag=driver.find_elements(By.XPATH,'//h3[@class=\"c-title  a-no-trucate a-font-primary-bold-s u-letter-spacing-0021 lrv-u-font-size-18@tablet lrv-u-font-size-16 u-line-height-125 u-line-height-normal@mobile-max a-truncate-ellipsis u-max-width-330 u-max-width-230@tablet-only\"]')\n",
    "for i in s_tag:\n",
    "    s=i.text\n",
    "    song.append(s)\n",
    "art_tag=driver.find_element(By.XPATH,'//span[@class=\"c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only u-font-size-20@tablet\"]')\n",
    "artist.append(art_tag.text)\n",
    "artist_tag=driver.find_elements(By.XPATH,'//span[@class=\"c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only\"]')\n",
    "for i in artist_tag:\n",
    "    a=i.text\n",
    "    artist.append(a)\n",
    "numbers=driver.find_elements(By.XPATH,'//span[@class=\"c-label  a-font-primary-bold-l a-font-primary-m@mobile-max u-font-weight-normal@mobile-max lrv-u-padding-tb-050@mobile-max u-font-size-32@tablet\"]')\n",
    "last_week.append(numbers[0].text)\n",
    "peak.append(numbers[1].text)\n",
    "weeks.append(numbers[2].text)\n",
    "numbers2=driver.find_elements(By.XPATH,'//span[@class=\"c-label  a-font-primary-m lrv-u-padding-tb-050@mobile-max\"]')\n",
    "for i in numbers2:\n",
    "    no2=i.text\n",
    "    data.append(no2)\n",
    "\n",
    "for i in range(int(len(data)/6)):\n",
    "    last_week.append(data[6*i])\n",
    "    peak.append(data[6*i+1])\n",
    "    weeks.append(data[6*i+2])\n",
    "len(weeks)\n",
    "df=pd.DataFrame({'Song name':song,'Artist name':artist,'Last week rank':last_week, 'Peak rank':peak,'Weeks on board':weeks})\n",
    "df\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "6.\n",
    "\n",
    "!pip install selenium\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "driver=webdriver.Chrome()\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "driver.maximize_window()\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "bookname=[]\n",
    "autname=[]\n",
    "vsold=[]\n",
    "publisher=[]\n",
    "bgenre=[]\n",
    " \n",
    "bkname=driver.find_elements(By.XPATH,\"//td[@class='left'][2]\")\n",
    "\n",
    "for i in bkname:\n",
    "    bkn=i.text\n",
    "    bookname.append(bkn)\n",
    "\n",
    "print(bookname)\n",
    "len(bookname)\n",
    "bautname=driver.find_elements(By.XPATH,\"//td[@class='left'][3]\")\n",
    "\n",
    "for i in bautname:\n",
    "    aut=i.text\n",
    "    autname.append(aut)\n",
    "\n",
    "print(autname)\n",
    "len(autname)\n",
    "bvsold=driver.find_elements(By.XPATH, \"//td[@class='left'][4]\")\n",
    "\n",
    "for i in bvsold:\n",
    "    vol=i.text\n",
    "    vsold.append(vol)\n",
    "\n",
    "print(vsold)\n",
    "len(vsold)\n",
    "bpublisher=driver.find_elements(By.XPATH,\"//td[@class='left'][5]\")\n",
    "\n",
    "for i in bpublisher:\n",
    "    pub=i.text\n",
    "    publisher.append(pub)\n",
    "\n",
    "print(publisher)\n",
    "len(publisher)\n",
    "bkgenre=driver.find_elements(By.XPATH,\"//td[@class='last left']\")\n",
    "\n",
    "for i in bkgenre:\n",
    "    gen=i.text\n",
    "    bgenre.append(gen)\n",
    "\n",
    "print(bgenre)\n",
    "len(bgenre)\n",
    "df=pd.DataFrame({'Book Name':bookname,'Author Name':autname,'Volumen Sold':vsold,'Publisher':publisher,'Genre':bgenre})\n",
    "df\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "7.\n",
    "\n",
    "!pip install selenium\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "driver=webdriver.Chrome()\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "driver.maximize_window()\n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "iname=[]\n",
    "year=[]\n",
    "genre=[]\n",
    "runtime=[]\n",
    "ratings=[]\n",
    "vites=[]\n",
    "iiname=driver.find_elements(By.XPATH,\"//h3[@class='lister-item-header']//a\")\n",
    "\n",
    "for i in iiname:\n",
    "    nm=i.text\n",
    "    iname.append(nm)\n",
    "\n",
    "print(iname)\n",
    "len(iname)\n",
    "iyear=driver.find_elements(By.XPATH,\"//h3[@class='lister-item-header']//span[2]\")\n",
    "\n",
    "for i in iyear:\n",
    "    yr=i.text\n",
    "    year.append(yr)\n",
    "\n",
    "print(year)\n",
    "len(year)\n",
    "igenre=driver.find_elements(By.XPATH,\"//span[@class='genre']\")\n",
    "\n",
    "for i in igenre:\n",
    "    ge=i.text\n",
    "    genre.append(ge)\n",
    "\n",
    "print(genre)\n",
    "len(genre)\n",
    "iruntime=driver.find_elements(By.XPATH, \"//span[@class='runtime']\")\n",
    "\n",
    "for i in iruntime:\n",
    "    te=i.text\n",
    "    runtime.append(te)\n",
    "\n",
    "print(runtime)\n",
    "len(runtime)\n",
    "iratings=driver.find_elements(By.XPATH, \"//div[@class='ipl-rating-star small']\")\n",
    "\n",
    "for i in iratings:\n",
    "    rg=i.text\n",
    "    ratings.append(rg)\n",
    "\n",
    "print(ratings)\n",
    "len(ratings)\n",
    "ivites=driver.find_elements(By.XPATH,\"//p[@class='text-muted text-small'][3]\")\n",
    "\n",
    "for i in ivites:\n",
    "    vs=i.text\n",
    "    vites.append(vs)\n",
    "\n",
    "print(vites)\n",
    "len(vites)\n",
    "df=pd.DataFrame({'Name':iname,'Year Span':year,'Genre':genre,'Run Time':runtime,'Ratings':ratings,'Votes':vites})\n",
    "df\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "8.\n",
    "\n",
    "driver.get('https://archive.ics.uci.edu/')\n",
    "\n",
    "driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div/div/div/a[1]').click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "dataset = []\n",
    "url = []\n",
    "\n",
    "for page in range(0,65):\n",
    "    data = driver.find_elements(By.XPATH,'//a[@class=\"link-hover link text-xl font-semibold\"]')\n",
    "    for i in data:\n",
    "        dataset.append(i.text)\n",
    "        url.append(i.get_attribute('href'))    \n",
    "    next_button = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[2]/div[3]/div/button[2]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "date =[]\n",
    "detail = []\n",
    "\n",
    "for i in url: \n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        dates = driver.find_element(By.XPATH,'//h2[@class=\"text-sm text-primary-content\"]')\n",
    "        date.append(dates.text.split('/'))\n",
    "    except NoSuchElementException:\n",
    "        date.append('nan')\n",
    "        \n",
    "for i in url: \n",
    "    driver.get(i)\n",
    "    time.sleep(5)        \n",
    "    details = driver.find_elements(By.XPATH,'//div[@class=\"col-span-4\"]')\n",
    "    for item in details:\n",
    "        detail.append(item.text)        \n",
    "year_date = pd.DataFrame(date,columns=['day','month','year'])\n",
    "d_type = []\n",
    "task = []\n",
    "f_type = []\n",
    "instance = []\n",
    "f_no = []\n",
    "\n",
    "for i in range(0,3894,6):\n",
    "    d_type.append(detail[i].replace('Dataset Characteristics\\n',''))\n",
    "    \n",
    "for i in range(2,3894,6):\n",
    "    task.append(detail[i].replace('Associated Tasks\\n',''))    \n",
    "\n",
    "for i in range(3,3894,6):\n",
    "    f_type.append(detail[i].replace('Feature Type\\n','')) \n",
    "    \n",
    "for i in range(4,3894,6):\n",
    "    instance.append(detail[i].replace('# Instances\\n',''))  \n",
    "    \n",
    "for i in range(5,3894,6):\n",
    "    f_no.append(detail[i].replace('# Features\\n','')) \n",
    "datasets = pd.DataFrame({'Dataset Name':dataset,'Data Type':d_type,'Associated Tasks':task,'Feature Type':f_type,'No. of Instances':instance,'No. of Features':f_no,'Year Donated':year_date['year']})\n",
    "datasets\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
